{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning classifier output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Click to see packages imported\"\n",
    "import os\n",
    "import configparser\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import wfdb\n",
    "import numpy as np\n",
    "import dsail\n",
    "from dsail.model.model_utils import get_model\n",
    "from dsail.train import Trainer\n",
    "from dsail.data import get_loss_weights_and_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current working directory is c:\\Users\\Shaun\\source\\Thesis\\MisdiagnosisOfAthleteECG\n"
     ]
    }
   ],
   "source": [
    "#|include: false\n",
    "# If the current working directory is the nbs/ folder, change to the project \n",
    "# root directory instead.\n",
    "\n",
    "if Path.cwd().stem == \"nbs\":\n",
    "    os.chdir(Path.cwd().parent)\n",
    "print(f\"The current working directory is {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: \"Click to see local packages imported\"\n",
    "from src.run_12ECG_classifier import load_12ECG_model, run_12ECG_classifier\n",
    "from src.data.util import get_all_records, get_predicted_findings, diagnosis_codes, codes_to_label_vector\n",
    "import src.data.norwegian as norwegian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets are located at C:\\Users\\Shaun\\source\\Thesis\\MisdiagnosisOfAthleteECG\\data\n"
     ]
    }
   ],
   "source": [
    "#|include: false\n",
    "# Import configuration settings, like location of data directory.\n",
    "config = configparser.ConfigParser()\n",
    "if not Path(\"config.ini\").exists():\n",
    "    print(\"WARNING: Please generate a config.ini file by running scripts/get_datasets.py\")\n",
    "else:\n",
    "    config.read(\"config.ini\")\n",
    "    data_dir = Path((config[\"datasets\"][\"path\"])).expanduser()\n",
    "    print(f\"Datasets are located at {data_dir.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only care about labels that correspond to \"borderline\" athlete findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinus_labels = [426177001, 426783006, 427084000, 427393009]\n",
    "# sinus_labels = [426177001, 426783006]       # Bradycardia or Normal\n",
    "rbbb_labels = [713427006, 713426002]        # Incomplete RBBB, Complete RBBB\n",
    "# won't do t-wave inversion, because no output for lead number provided.\n",
    "athlete_labels = sinus_labels + rbbb_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_weights_dir = Path.cwd() / \"checkpoints\" / \"original\"\n",
    "finetune_dir = Path.cwd() / \"checkpoints\" / \"finetune_2\"\n",
    "\n",
    "config_dir = Path.cwd() / \"config\"\n",
    "training_data_dir = data_dir / \"challenge-2020\" / \"1.0.2\" / \"training\"\n",
    "target_data_dir = data_dir / \"norwegian-athlete-ecg\" / \"1.0.0\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "if finetune_dir.exists():\n",
    "    print(f\"{finetune_dir} already exists. Are we overwriting an existing finetune?\")\n",
    "else:\n",
    "    finetune_dir.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model config from disk\n",
    "data_cfg = dsail.config.DataConfig(config_dir / \"data.json\")\n",
    "preprocess_cfg = dsail.config.PreprocessConfig(config_dir / \"preprocess.json\")\n",
    "model_cfg = dsail.config.ModelConfig(config_dir / \"model.json\")\n",
    "run_cfg = dsail.config.RunConfig(config_dir / \"run.json\")\n",
    "\n",
    "# Check if CUDA device available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrowed from DSAIL_SNU\n",
    "\n",
    "def set_seeds(seed):\n",
    "    \"\"\" set random seeds \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 12ECG model...\n"
     ]
    }
   ],
   "source": [
    "print('Loading 12ECG model...')\n",
    "model = load_12ECG_model(original_weights_dir, config_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model is an ensemble of 10 networks\n"
     ]
    }
   ],
   "source": [
    "print(f\"This model is an ensemble of {len(model[3])} networks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 256])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each classification output, there are weights for 256 inputs to adjust.\n",
    "# Can we use partial backpropogation for this layer only?\n",
    "model[3][0].linear.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternatively, is just adding bias enough?\n",
    "model[3][0].linear.bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only want the linear layer at the output of each network to be trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping cell\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping cell\n",
    "# Only classifier output stem should be trainable\n",
    "# https://stackoverflow.com/questions/62523912/freeze-certain-layers-of-an-existing-model-in-pytorch\n",
    "\n",
    "# Apply to all 10 networks in ensemble model\n",
    "for net in model[3]:\n",
    "    # Freeze all parameters in model\n",
    "    for param in net.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Unfreeze linear layer from output stem (classifier output)\n",
    "    for param in net.linear.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping cell\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping cell\n",
    "#|include: false\n",
    "# Deprecated, keeping the carcass in case I need to use Trainer class again.\n",
    "\n",
    "from src.train import evaluate, train\n",
    "from dsail.data import collate_into_block\n",
    "\n",
    "loss_fn = torch.nn.L1Loss()\n",
    "\n",
    "for entry in get_all_records(target_data_dir):\n",
    "    # 12-lead ECG signals (input data), and header info (e.g. sampling frequency)\n",
    "    record = wfdb.rdrecord(target_data_dir / entry)\n",
    "    signals = record.p_signal.transpose()\n",
    "    with open((target_data_dir / entry).with_suffix(\".hea\"), 'r') as f:\n",
    "        header_data=f.readlines()\n",
    "\n",
    "    # Actual labels from cardiologist\n",
    "    comments_c = record.comments[1]\n",
    "    findings_c = norwegian.extract_findings(comments_c)\n",
    "    actual_findings = norwegian.classify_relevant_findings(findings_c)\n",
    "    actual_labels = codes_to_label_vector(actual_findings, athlete_labels)\n",
    "    actual_scores = np.array(actual_labels, dtype=float)\n",
    "\n",
    "    # Run full model, get predictions\n",
    "    current_label, current_score, classes = run_12ECG_classifier(signals, header_data, model)\n",
    "    predicted_scores = np.zeros(len(athlete_labels))\n",
    "    for i, code in enumerate(classes):\n",
    "        if int(code) in athlete_labels:\n",
    "            index = athlete_labels.index(int(code))\n",
    "            predicted_scores[index] = current_score[i]\n",
    "    \n",
    "    # Run each ensemble model separately\n",
    "    losses = []\n",
    "    for net in model[3]:\n",
    "        # Setup for using DSAIL_SNU Trainer object for eval\n",
    "        data_cfg.data = signals\n",
    "        data_cfg.header = header_data\n",
    "        dataset_val = dsail.data.get_dataset_from_configs(data_cfg, preprocess_cfg)\n",
    "        iterator_val = torch.utils.data.DataLoader(dataset_val, 1, collate_fn=dsail.data.collate_into_list)\n",
    "        dataset_train = dsail.data.get_dataset_from_configs(data_cfg, preprocess_cfg)\n",
    "        iterator_train = torch.utils.data.DataLoader(dataset_train, 1, collate_fn=dsail.data.collate_into_list)\n",
    "\n",
    "        loss_weights_and_flags = get_loss_weights_and_flags(data_cfg, run_cfg)\n",
    "        # trainer = Trainer(net, data_cfg, run_cfg.multilabel, loss_weights_and_flags)\n",
    "        # trainer.set_device(device, data_parallel=False)\n",
    "        # trainer.set_optim_scheduler(run_cfg, list(net.parameters()))\n",
    "        optimizer = torch.optim.SGD(params=net.parameters(), lr=0.01)\n",
    "\n",
    "        # Evaluate model (forward pass)\n",
    "        # If signal is too long, it'll get cut into multiple batches\n",
    "        # for batch in iterator_val:\n",
    "        #     trainer.evaluate(batch)\n",
    "        # predicted_scores_net = np.zeros(len(athlete_labels))\n",
    "        # for i, code in enumerate(classes):\n",
    "        #     if int(code) in athlete_labels:\n",
    "        #         index = athlete_labels.index(int(code))\n",
    "        #         predicted_scores_net[index] = trainer.logger_eval.scalar_outputs[0][0][i]\n",
    "        \n",
    "        scalar_outputs = None\n",
    "        for batch in iterator_train:\n",
    "            # scalar_outputs = evaluate(net, batch, device, loss_weights_and_flags)\n",
    "            scalar_outputs, ce_loss = train(net, batch, device, optimizer, classes, athlete_labels, actual_scores)\n",
    "        # trainer.scheduler_step()\n",
    "        predicted_scores_net = np.zeros(len(athlete_labels))\n",
    "        for i, code in enumerate(classes):\n",
    "            if int(code) in athlete_labels:\n",
    "                index = athlete_labels.index(int(code))\n",
    "                predicted_scores_net[index] = scalar_outputs[0][i]\n",
    "                # predicted_scores_net[index] = trainer.logger_train.scalar_outputs[0][0][i]\n",
    "\n",
    "        # Find error\n",
    "        loss = loss_fn(torch.Tensor(predicted_scores_net), torch.Tensor(actual_scores))\n",
    "        losses.append(loss)\n",
    "\n",
    "        # # \n",
    "        # optimizer.zero_grad()\n",
    "        # loss.backward()\n",
    "\n",
    "            \n",
    "    # Find error (for full model)\n",
    "    loss = loss_fn(torch.Tensor(predicted_scores), torch.Tensor(actual_scores))\n",
    "    print(f\"loss (full model) = {loss}\")\n",
    "    print(f\"loss (avg of 10) = {sum(losses) / len(losses)}\")\n",
    "\n",
    "    # Adjust output stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of class labels returned by classifier\n",
    "# current_label, current_score, classes = run_12ECG_classifier(signals, header_data, model)\n",
    "classes = [\n",
    "    '10370003',\n",
    "    '111975006',\n",
    "    '164889003',\n",
    "    '164890007',\n",
    "    '164909002',\n",
    "    '164917005',\n",
    "    '164934002',\n",
    "    '164947007',\n",
    "    '251146004',\n",
    "    '270492004',\n",
    "    '284470004',\n",
    "    '39732003',\n",
    "    '426177001',\n",
    "    '426627000',\n",
    "    '426783006',\n",
    "    '427084000',\n",
    "    '427172004',\n",
    "    '427393009',\n",
    "    '445118002',\n",
    "    '47665007',\n",
    "    '59931005',\n",
    "    '698252002',\n",
    "    '713426002',\n",
    "    '713427006'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping cell\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping cell\n",
    "# Evaluate original model\n",
    "\n",
    "loss_fn = torch.nn.L1Loss()\n",
    "\n",
    "for entry in get_all_records(target_data_dir):\n",
    "    # 12-lead ECG signals (input data), and header info (e.g. sampling frequency)\n",
    "    record = wfdb.rdrecord(target_data_dir / entry)\n",
    "    signals = record.p_signal.transpose()\n",
    "    with open((target_data_dir / entry).with_suffix(\".hea\"), 'r') as f:\n",
    "        header_data=f.readlines()\n",
    "\n",
    "    # Actual labels from cardiologist\n",
    "    comments_c = record.comments[1]\n",
    "    findings_c = norwegian.extract_findings(comments_c)\n",
    "    actual_findings = norwegian.classify_relevant_findings(findings_c)\n",
    "    actual_labels = codes_to_label_vector(actual_findings, athlete_labels)\n",
    "    actual_scores = np.array(actual_labels, dtype=float)\n",
    "\n",
    "    # Run full model, get predictions\n",
    "    current_label, current_score, classes = run_12ECG_classifier(signals, header_data, model)\n",
    "    predicted_scores = np.zeros(len(athlete_labels))\n",
    "    for i, code in enumerate(classes):\n",
    "        if int(code) in athlete_labels:\n",
    "            index = athlete_labels.index(int(code))\n",
    "            predicted_scores[index] = current_score[i]\n",
    "    \n",
    "    # Find error (for full model)\n",
    "    loss = loss_fn(torch.Tensor(predicted_scores), torch.Tensor(actual_scores))\n",
    "    print(f\"loss (full model) = {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuning network 1 / 10 over 100 epochs\n",
      "\tinit (epoch 0): \t1.6440274715423584\n",
      "\tbest (epoch 1): \t1.6220239400863647\n",
      "\timprovement: 0.022003531455993652\n",
      "Finetuning network 2 / 10 over 100 epochs\n",
      "\tinit (epoch 0): \t1.6619373559951782\n",
      "\tbest (epoch 60): \t1.6334235668182373\n",
      "\timprovement: 0.028513789176940918\n",
      "Finetuning network 3 / 10 over 100 epochs\n",
      "\tinit (epoch 0): \t1.6975668668746948\n",
      "\tbest (epoch 81): \t1.6526380777359009\n",
      "\timprovement: 0.044928789138793945\n",
      "Finetuning network 4 / 10 over 100 epochs\n",
      "\tinit (epoch 0): \t1.6788705587387085\n",
      "\tbest (epoch 88): \t1.6343796253204346\n",
      "\timprovement: 0.044490933418273926\n",
      "Finetuning network 5 / 10 over 100 epochs\n",
      "\tinit (epoch 0): \t1.6671278476715088\n",
      "\tbest (epoch 9): \t1.623307466506958\n",
      "\timprovement: 0.04382038116455078\n",
      "Finetuning network 6 / 10 over 100 epochs\n",
      "\tinit (epoch 0): \t1.6786848306655884\n",
      "\tbest (epoch 36): \t1.6391350030899048\n",
      "\timprovement: 0.039549827575683594\n",
      "Finetuning network 7 / 10 over 100 epochs\n",
      "\tinit (epoch 0): \t1.6817437410354614\n",
      "\tbest (epoch 70): \t1.652164101600647\n",
      "\timprovement: 0.029579639434814453\n",
      "Finetuning network 8 / 10 over 100 epochs\n",
      "\tinit (epoch 0): \t1.653768539428711\n",
      "\tbest (epoch 45): \t1.6203107833862305\n",
      "\timprovement: 0.03345775604248047\n",
      "Finetuning network 9 / 10 over 100 epochs\n",
      "\tinit (epoch 0): \t1.66851007938385\n",
      "\tbest (epoch 55): \t1.6355277299880981\n",
      "\timprovement: 0.03298234939575195\n",
      "Finetuning network 10 / 10 over 100 epochs\n",
      "\tinit (epoch 0): \t1.6196333169937134\n",
      "\tbest (epoch 85): \t1.6157557964324951\n",
      "\timprovement: 0.0038775205612182617\n"
     ]
    }
   ],
   "source": [
    "from src.train import evaluate, train\n",
    "\n",
    "N_epochs = 100\n",
    "\n",
    "# Train each of the 10 networks separately\n",
    "# What does \"fold\" mean in this context?\n",
    "for fold, net in enumerate(model[3]):\n",
    "    print(f\"Finetuning network {fold+1} / 10 over {N_epochs} epochs\")\n",
    "    set_seeds(2020)\n",
    "    optimizer = torch.optim.SGD(params=net.parameters(), lr=0.0001)\n",
    "    avg_ce_losses = []\n",
    "    for epoch in range(N_epochs):\n",
    "        ce_losses = []\n",
    "        for entry in get_all_records(target_data_dir):      # TODO: Split into train and validation subsets\n",
    "            # Inputs\n",
    "            # 12-lead ECG signals (input data), and header info (e.g. sampling frequency)\n",
    "            record = wfdb.rdrecord(target_data_dir / entry)\n",
    "            signals = record.p_signal.transpose()\n",
    "            with open((target_data_dir / entry).with_suffix(\".hea\"), 'r') as f:\n",
    "                header_data=f.readlines()\n",
    "\n",
    "            # True labels from cardiologist\n",
    "            comments_c = record.comments[1]\n",
    "            findings_c = norwegian.extract_findings(comments_c)\n",
    "            actual_findings = norwegian.classify_relevant_findings(findings_c)\n",
    "            actual_labels = codes_to_label_vector(actual_findings, athlete_labels)\n",
    "            actual_scores = np.array(actual_labels, dtype=float)\n",
    "\n",
    "            # Load data for training\n",
    "            data_cfg.data = signals\n",
    "            data_cfg.header = header_data\n",
    "            dataset_train = dsail.data.get_dataset_from_configs(data_cfg, preprocess_cfg)\n",
    "            iterator_train = torch.utils.data.DataLoader(dataset_train, 1, collate_fn=dsail.data.collate_into_list)\n",
    "\n",
    "            # Training pass\n",
    "            # 1. Forward pass\n",
    "            # 2. Cross-entropy loss\n",
    "            # 3. Zero gradients\n",
    "            # 4. Backpropagation on loss\n",
    "            # 5. Gradient descent\n",
    "            scalar_outputs = None\n",
    "            for batch in iterator_train:\n",
    "                # scalar_outputs = evaluate(net, batch, device, loss_weights_and_flags)\n",
    "                scalar_outputs, ce_loss = train(net, batch, device, optimizer, classes, athlete_labels, actual_scores)\n",
    "                ce_losses.append(ce_loss)\n",
    "\n",
    "            # Select athletic subset of predicted scores for later analysis\n",
    "            predicted_scores_net = np.zeros(len(athlete_labels))\n",
    "            for i, code in enumerate(classes):\n",
    "                if int(code) in athlete_labels:\n",
    "                    index = athlete_labels.index(int(code))\n",
    "                    predicted_scores_net[index] = scalar_outputs[0][i]\n",
    "        \n",
    "        avg_ce_losses.append(sum(ce_losses) / len(ce_losses))\n",
    "\n",
    "        # Save checkpoint\n",
    "        torch.save(net.state_dict(), finetune_dir / f\"model_{fold}_epoch_{epoch}.sav\")\n",
    "    \n",
    "    print(f\"\\tinit (epoch 0): \\t{avg_ce_losses[0]}\")\n",
    "    print(f\"\\tbest (epoch {avg_ce_losses.index(min(avg_ce_losses))}): \\t{min(avg_ce_losses)}\")\n",
    "    print(f\"\\timprovement: {avg_ce_losses[0] - min(avg_ce_losses)}\")\n",
    "    \n",
    "    # Save best network weights\n",
    "    shutil.copyfile(\n",
    "        finetune_dir / f\"model_{fold}_epoch_{avg_ce_losses.index(min(avg_ce_losses))}.sav\", \n",
    "        finetune_dir / f\"finalized_model_{fold}.sav\"\n",
    "    )\n",
    "    \n",
    "    # Copy original class thresholds\n",
    "    shutil.copyfile(\n",
    "        original_weights_dir / f\"finalized_model_thresholds_{fold}.npy\",\n",
    "        finetune_dir / f\"finalized_model_thresholds_{fold}.npy\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading finetuned model\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading finetuned model\")\n",
    "model_finetune = load_12ECG_model(finetune_dir, config_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss (original) = 1.6488487720489502\tloss (finetune) = 1.6584820747375488\n",
      "[0. 0. 0. 1. 0. 0.]\n",
      "[0.01571385 0.57213237 0.0145252  0.35925963 0.15850817 0.04178687]\n",
      "[0.01418421 0.54637196 0.01346938 0.34263794 0.17458124 0.04220469]\n",
      "loss (original) = 1.57094144821167\tloss (finetune) = 1.5903799533843994\n",
      "[0. 0. 0. 1. 0. 0.]\n",
      "[0.04767237 0.74101483 0.01472449 0.48700721 0.01970278 0.02061614]\n",
      "[0.04414863 0.73619656 0.01411685 0.46071363 0.02058608 0.02280122]\n",
      "loss (original) = 1.8113987445831299\tloss (finetune) = 1.8125780820846558\n",
      "[0. 1. 0. 0. 0. 0.]\n",
      "[0.01351097 0.15493916 0.02472729 0.07357612 0.55262998 0.11695744]\n",
      "[0.01310635 0.1527068  0.02221194 0.06393385 0.55728036 0.11722222]\n",
      "loss (original) = 1.268202781677246\tloss (finetune) = 1.2776403427124023\n",
      "[0. 1. 0. 0. 0. 0.]\n",
      "[0.0723825  0.7378892  0.04833657 0.04445688 0.07290736 0.09258826]\n",
      "[0.06857638 0.72607492 0.04551834 0.04085563 0.07762357 0.10403387]\n",
      "loss (original) = 3.1200459003448486\tloss (finetune) = 3.1249308586120605\n",
      "[1. 1. 0. 0. 0. 0.]\n",
      "[0.60250697 0.26694095 0.02284611 0.12823623 0.03309979 0.02381691]\n",
      "[0.59048248 0.26749004 0.02094831 0.12459922 0.03494981 0.02487893]\n",
      "loss (original) = 1.797753095626831\tloss (finetune) = 1.8013739585876465\n",
      "[0. 1. 0. 0. 0. 0.]\n",
      "[0.64141081 0.19216686 0.05224815 0.09503932 0.03384928 0.01601934]\n",
      "[0.62784805 0.18217347 0.04972834 0.08645914 0.03599369 0.01699816]\n",
      "loss (original) = 1.6284942626953125\tloss (finetune) = 1.6463735103607178\n",
      "[0. 1. 0. 0. 0. 0.]\n",
      "[0.01000513 0.44488331 0.03766386 0.04548306 0.22730712 0.71350882]\n",
      "[0.00916525 0.42275468 0.03732985 0.04412423 0.25204625 0.69949999]\n",
      "loss (original) = 2.811042547225952\tloss (finetune) = 2.8272008895874023\n",
      "[0. 1. 0. 0. 0. 1.]\n",
      "[0.0037526  0.65832854 0.03732894 0.12537394 0.16770522 0.83033781]\n",
      "[0.00336688 0.64314663 0.03527044 0.11738343 0.18292815 0.81699419]\n",
      "loss (original) = 1.6097058057785034\tloss (finetune) = 1.6284849643707275\n",
      "[0. 1. 0. 0. 0. 0.]\n",
      "[0.07960815 0.2715372  0.05862192 0.04279659 0.0263374  0.03539298]\n",
      "[0.07027754 0.24618988 0.05634032 0.03786081 0.02938169 0.03959475]\n",
      "loss (original) = 1.3340107202529907\tloss (finetune) = 1.339659333229065\n",
      "[0. 1. 0. 0. 0. 0.]\n",
      "[0.11997518 0.69201933 0.05226045 0.02866011 0.06737801 0.26718328]\n",
      "[0.11775481 0.68734637 0.05053276 0.02751459 0.07078492 0.28129389]\n",
      "loss (original) = 1.6833950281143188\tloss (finetune) = 1.6950410604476929\n",
      "[0. 0. 0. 1. 0. 0.]\n",
      "[0.00410477 0.91153256 0.0127469  0.39949349 0.02153702 0.00966191]\n",
      "[0.00368275 0.90853043 0.01231845 0.38423048 0.02269531 0.01038836]\n",
      "loss (original) = 1.2226649522781372\tloss (finetune) = 1.231184482574463\n",
      "[0. 0. 0. 1. 0. 0.]\n",
      "[0.07495635 0.20974713 0.01665777 0.80709539 0.02193894 0.02048079]\n",
      "[0.06631043 0.20855413 0.0157494  0.79314913 0.02281005 0.02109378]\n",
      "loss (original) = 1.4569779634475708\tloss (finetune) = 1.4846789836883545\n",
      "[1. 0. 0. 0. 0. 0.]\n",
      "[0.54024608 0.13898879 0.0371703  0.09451715 0.25557156 0.07216764]\n",
      "[0.50601645 0.12425165 0.03439711 0.08666344 0.28425871 0.07264603]\n",
      "loss (original) = 1.3580752611160278\tloss (finetune) = 1.3661832809448242\n",
      "[0. 1. 0. 0. 0. 0.]\n",
      "[0.00400075 0.59680797 0.02995681 0.020625   0.15185982 0.02715511]\n",
      "[0.00352498 0.58613175 0.02796917 0.0186958  0.15470021 0.02944159]\n",
      "loss (original) = 1.346266746520996\tloss (finetune) = 1.35075044631958\n",
      "[0. 1. 0. 0. 0. 0.]\n",
      "[0.28710761 0.66902727 0.04387987 0.11507727 0.02873761 0.02181714]\n",
      "[0.29060277 0.66257422 0.04054436 0.10886104 0.03098547 0.02303646]\n",
      "loss (original) = 1.5490772724151611\tloss (finetune) = 1.5522602796554565\n",
      "[0. 1. 0. 0. 0. 0.]\n",
      "[0.28920273 0.3933781  0.04631206 0.04246618 0.05125916 0.01502604]\n",
      "[0.26904261 0.38418867 0.04457202 0.03990866 0.05325105 0.01583395]\n",
      "loss (original) = 1.3151956796646118\tloss (finetune) = 1.3251322507858276\n",
      "[0. 1. 0. 0. 0. 0.]\n",
      "[0.00514905 0.65487469 0.02518368 0.12916211 0.01590612 0.05987683]\n",
      "[0.0046712  0.64033343 0.02324332 0.1261448  0.01689688 0.05962459]\n",
      "loss (original) = 1.6431007385253906\tloss (finetune) = 1.6546869277954102\n",
      "[0. 0. 0. 1. 0. 0.]\n",
      "[0.25286235 0.54134577 0.02530998 0.3808419  0.05424296 0.01704435]\n",
      "[0.24932864 0.5322409  0.0228781  0.36404075 0.06070714 0.01781123]\n",
      "loss (original) = 1.1706494092941284\tloss (finetune) = 1.1732734441757202\n",
      "[0. 0. 0. 1. 0. 0.]\n",
      "[0.00566916 0.57669329 0.00526076 0.96014779 0.01182769 0.00647336]\n",
      "[0.0052056  0.57939354 0.00493843 0.9571043  0.01211677 0.00656358]\n",
      "loss (original) = 1.0891989469528198\tloss (finetune) = 1.0863730907440186\n",
      "[0. 1. 0. 0. 0. 0.]\n",
      "[0.00339676 0.97740301 0.01453176 0.1784937  0.01787076 0.00886783]\n",
      "[0.00300726 0.97780338 0.01338092 0.16242983 0.01792456 0.00921941]\n",
      "loss (original) = 1.945864200592041\tloss (finetune) = 1.9516974687576294\n",
      "[0. 0. 0. 1. 0. 0.]\n",
      "[0.75590764 0.23247245 0.03276824 0.16557923 0.26017668 0.30626999]\n",
      "[0.73867642 0.23176384 0.03042081 0.15151906 0.26320706 0.29483846]\n",
      "loss (original) = 1.2134252786636353\tloss (finetune) = 1.2162461280822754\n",
      "[0. 1. 0. 0. 0. 0.]\n",
      "[0.00393622 0.9193598  0.0328863  0.05844019 0.10002841 0.54690099]\n",
      "[0.00361338 0.9141732  0.03100571 0.05477102 0.10844936 0.54100077]\n",
      "loss (original) = 1.6690798997879028\tloss (finetune) = 1.6940302848815918\n",
      "[1. 0. 0. 0. 0. 0.]\n",
      "[0.2617553  0.31334034 0.02864672 0.08651818 0.07639904 0.02882603]\n",
      "[0.22732949 0.30201095 0.02746281 0.07687722 0.07954399 0.03137605]\n",
      "loss (original) = 3.5190815925598145\tloss (finetune) = 3.564112663269043\n",
      "[1. 0. 0. 0. 0. 1.]\n",
      "[0.3560622  0.09907028 0.02555196 0.06146489 0.82190052 0.36672798]\n",
      "[0.31259198 0.09992814 0.02240605 0.05199734 0.83705006 0.35027955]\n",
      "loss (original) = 1.1600233316421509\tloss (finetune) = 1.1572754383087158\n",
      "[0. 1. 0. 0. 0. 0.]\n",
      "[0.07555088 0.87223398 0.03231608 0.03197179 0.07692201 0.01516041]\n",
      "[0.07419069 0.87544118 0.03103917 0.02965978 0.076674   0.01639762]\n",
      "loss (original) = 1.2278964519500732\tloss (finetune) = 1.2427775859832764\n",
      "[0. 1. 0. 0. 0. 0.]\n",
      "[0.00511518 0.89675966 0.03785176 0.0458353  0.11336476 0.53516178]\n",
      "[0.0047725  0.88079942 0.03514336 0.04493239 0.13171753 0.54282916]\n",
      "loss (original) = 1.8033802509307861\tloss (finetune) = 1.8322359323501587\n",
      "[1. 0. 0. 0. 0. 0.]\n",
      "[0.23314607 0.33441133 0.0310402  0.03003324 0.61886288 0.08197292]\n",
      "[0.20518979 0.32314021 0.02786364 0.02543567 0.65365549 0.0816228 ]\n",
      "loss (original) = 1.7239793539047241\tloss (finetune) = 1.7363264560699463\n",
      "[0. 1. 0. 0. 0. 0.]\n",
      "[0.52684148 0.31289762 0.03165959 0.03495285 0.37810827 0.07549243]\n",
      "[0.53135756 0.30315816 0.03007273 0.03125174 0.40155866 0.07218213]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate finetuned model\n",
    "\n",
    "# loss_fn = torch.nn.L1Loss()     # TODO: Should use cross-entropy loss for multiclass classifier\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for entry in get_all_records(target_data_dir):\n",
    "    # 12-lead ECG signals (input data), and header info (e.g. sampling frequency)\n",
    "    record = wfdb.rdrecord(target_data_dir / entry)\n",
    "    signals = record.p_signal.transpose()\n",
    "    with open((target_data_dir / entry).with_suffix(\".hea\"), 'r') as f:\n",
    "        header_data=f.readlines()\n",
    "\n",
    "    # Actual labels from cardiologist\n",
    "    comments_c = record.comments[1]\n",
    "    findings_c = norwegian.extract_findings(comments_c)\n",
    "    actual_findings = norwegian.classify_relevant_findings(findings_c)\n",
    "    actual_labels = codes_to_label_vector(actual_findings, athlete_labels)\n",
    "    actual_scores = np.array(actual_labels, dtype=float)\n",
    "\n",
    "    # Run original model, get predictions\n",
    "    current_label, current_score, classes = run_12ECG_classifier(signals, header_data, model)\n",
    "    predicted_scores = np.zeros(len(athlete_labels))\n",
    "    for i, code in enumerate(classes):\n",
    "        if int(code) in athlete_labels:\n",
    "            index = athlete_labels.index(int(code))\n",
    "            predicted_scores[index] = current_score[i]\n",
    "    \n",
    "    # Run finetuned model, get predictions\n",
    "    current_label, current_score, classes = run_12ECG_classifier(signals, header_data, model_finetune)\n",
    "    predicted_scores_finetune = np.zeros(len(athlete_labels))\n",
    "    for i, code in enumerate(classes):\n",
    "        if int(code) in athlete_labels:\n",
    "            index = athlete_labels.index(int(code))\n",
    "            predicted_scores_finetune[index] = current_score[i]\n",
    "    \n",
    "    # Find error (for full model)\n",
    "    loss_original = loss_fn(torch.Tensor(predicted_scores), torch.Tensor(actual_scores))\n",
    "    loss_finetune = loss_fn(torch.Tensor(predicted_scores_finetune), torch.Tensor(actual_scores))\n",
    "    print(f\"loss (original) = {loss_original}\\tloss (finetune) = {loss_finetune}\")\n",
    "    print(actual_scores)\n",
    "    print(predicted_scores)\n",
    "    print(predicted_scores_finetune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m\n",
      "\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mparams\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mIterable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mlr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmomentum\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdampening\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mweight_decay\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mnesterov\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmaximize\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mforeach\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdifferentiable\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mfused\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m     \n",
      "Implements stochastic gradient descent (optionally with momentum).\n",
      "\n",
      ".. math::\n",
      "   \\begin{aligned}\n",
      "        &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
      "        &\\textbf{input}      : \\gamma \\text{ (lr)}, \\: \\theta_0 \\text{ (params)}, \\: f(\\theta)\n",
      "            \\text{ (objective)}, \\: \\lambda \\text{ (weight decay)},                          \\\\\n",
      "        &\\hspace{13mm} \\:\\mu \\text{ (momentum)}, \\:\\tau \\text{ (dampening)},\n",
      "        \\:\\textit{ nesterov,}\\:\\textit{ maximize}                                     \\\\[-1.ex]\n",
      "        &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
      "        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n",
      "        &\\hspace{5mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})           \\\\\n",
      "        &\\hspace{5mm}\\textbf{if} \\: \\lambda \\neq 0                                           \\\\\n",
      "        &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda  \\theta_{t-1}                            \\\\\n",
      "        &\\hspace{5mm}\\textbf{if} \\: \\mu \\neq 0                                               \\\\\n",
      "        &\\hspace{10mm}\\textbf{if} \\: t > 1                                                   \\\\\n",
      "        &\\hspace{15mm} \\textbf{b}_t \\leftarrow \\mu \\textbf{b}_{t-1} + (1-\\tau) g_t           \\\\\n",
      "        &\\hspace{10mm}\\textbf{else}                                                          \\\\\n",
      "        &\\hspace{15mm} \\textbf{b}_t \\leftarrow g_t                                           \\\\\n",
      "        &\\hspace{10mm}\\textbf{if} \\: \\textit{nesterov}                                       \\\\\n",
      "        &\\hspace{15mm} g_t \\leftarrow g_{t} + \\mu \\textbf{b}_t                             \\\\\n",
      "        &\\hspace{10mm}\\textbf{else}                                                   \\\\[-1.ex]\n",
      "        &\\hspace{15mm} g_t  \\leftarrow  \\textbf{b}_t                                         \\\\\n",
      "        &\\hspace{5mm}\\textbf{if} \\: \\textit{maximize}                                          \\\\\n",
      "        &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} + \\gamma g_t                   \\\\[-1.ex]\n",
      "        &\\hspace{5mm}\\textbf{else}                                                    \\\\[-1.ex]\n",
      "        &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma g_t                   \\\\[-1.ex]\n",
      "        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
      "        &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n",
      "        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
      "   \\end{aligned}\n",
      "\n",
      "Nesterov momentum is based on the formula from\n",
      "`On the importance of initialization and momentum in deep learning`__.\n",
      "\n",
      "Args:\n",
      "    params (iterable): iterable of parameters or named_parameters to optimize\n",
      "        or iterable of dicts defining parameter groups. When using named_parameters,\n",
      "        all parameters in all groups should be named\n",
      "    lr (float, Tensor, optional): learning rate (default: 1e-3)\n",
      "    momentum (float, optional): momentum factor (default: 0)\n",
      "    dampening (float, optional): dampening for momentum (default: 0)\n",
      "    weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
      "    nesterov (bool, optional): enables Nesterov momentum. Only applicable\n",
      "        when momentum is non-zero. (default: False)\n",
      "    maximize (bool, optional): maximize the objective with respect to the\n",
      "        params, instead of minimizing (default: False)\n",
      "    foreach (bool, optional): whether foreach implementation of optimizer\n",
      "        is used. If unspecified by the user (so foreach is None), we will try to use\n",
      "        foreach over the for-loop implementation on CUDA, since it is usually\n",
      "        significantly more performant. Note that the foreach implementation uses\n",
      "        ~ sizeof(params) more peak memory than the for-loop version due to the intermediates\n",
      "        being a tensorlist vs just one tensor. If memory is prohibitive, batch fewer\n",
      "        parameters through the optimizer at a time or switch this flag to False (default: None)\n",
      "    differentiable (bool, optional): whether autograd should\n",
      "        occur through the optimizer step in training. Otherwise, the step()\n",
      "        function runs in a torch.no_grad() context. Setting to True can impair\n",
      "        performance, so leave it False if you don't intend to run autograd\n",
      "        through this instance (default: False)\n",
      "    fused (bool, optional): whether the fused implementation is used.\n",
      "        Currently, `torch.float64`, `torch.float32`, `torch.float16`, and `torch.bfloat16`\n",
      "        are supported. (default: None)\n",
      "\n",
      ".. note:: The foreach and fused implementations are typically faster than the for-loop,\n",
      "          single-tensor implementation, with fused being theoretically fastest with both\n",
      "          vertical and horizontal fusion. As such, if the user has not specified either\n",
      "          flag (i.e., when foreach = fused = None), we will attempt defaulting to the foreach\n",
      "          implementation when the tensors are all on CUDA. Why not fused? Since the fused\n",
      "          implementation is relatively new, we want to give it sufficient bake-in time.\n",
      "          To specify fused, pass True for fused. To force running the for-loop\n",
      "          implementation, pass False for either foreach or fused. \n",
      "\n",
      "\n",
      "Example:\n",
      "    >>> # xdoctest: +SKIP\n",
      "    >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
      "    >>> optimizer.zero_grad()\n",
      "    >>> loss_fn(model(input), target).backward()\n",
      "    >>> optimizer.step()\n",
      "\n",
      "__ http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf\n",
      "\n",
      ".. note::\n",
      "    The implementation of SGD with Momentum/Nesterov subtly differs from\n",
      "    Sutskever et al. and implementations in some other frameworks.\n",
      "\n",
      "    Considering the specific case of Momentum, the update can be written as\n",
      "\n",
      "    .. math::\n",
      "        \\begin{aligned}\n",
      "            v_{t+1} & = \\mu * v_{t} + g_{t+1}, \\\\\n",
      "            p_{t+1} & = p_{t} - \\text{lr} * v_{t+1},\n",
      "        \\end{aligned}\n",
      "\n",
      "    where :math:`p`, :math:`g`, :math:`v` and :math:`\\mu` denote the\n",
      "    parameters, gradient, velocity, and momentum respectively.\n",
      "\n",
      "    This is in contrast to Sutskever et al. and\n",
      "    other frameworks which employ an update of the form\n",
      "\n",
      "    .. math::\n",
      "        \\begin{aligned}\n",
      "            v_{t+1} & = \\mu * v_{t} + \\text{lr} * g_{t+1}, \\\\\n",
      "            p_{t+1} & = p_{t} - v_{t+1}.\n",
      "        \\end{aligned}\n",
      "\n",
      "    The Nesterov version is analogously modified.\n",
      "\n",
      "    Moreover, the initial value of the momentum buffer is set to the\n",
      "    gradient value at the first step. This is in contrast to some other\n",
      "    frameworks that initialize it to all zeros.\n",
      "\u001b[1;31mFile:\u001b[0m           c:\\users\\shaun\\source\\thesis\\misdiagnosisofathleteecg\\.venv\\lib\\site-packages\\torch\\optim\\sgd.py\n",
      "\u001b[1;31mType:\u001b[0m           type\n",
      "\u001b[1;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "torch.optim.SGD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.220446049250313e-16\n"
     ]
    }
   ],
   "source": [
    "print(np.finfo(float).eps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
