{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance between probability distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choices\n",
    "from math import log2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfair coin example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example inspired from [Adian Liusie video](https://www.youtube.com/watch?v=SxGYPqCgJWM)\n",
    "\n",
    "Let's say we have two coins: a fair coin (50/50 chance of heads or tails) and \n",
    "an unfair coin (80/20 chance of heads or tails). We can model the outcome of \n",
    "flipping the coins using random variables $X_\\text{fair}$ and $X_\\text{unfair}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fair coin:\tP(X=Heads) 0.5\n",
      "Unfair coin:\tP(X=Heads) 0.78\n"
     ]
    }
   ],
   "source": [
    "# Define probability distributions\n",
    "events = ['H', 'T']\n",
    "p_fair = [0.5, 0.5]\n",
    "p_unfair = [0.8, 0.2]\n",
    "\n",
    "# Simulate 100 coin flips\n",
    "N = 100\n",
    "fair_dataset = choices(events, p_fair, k=N)\n",
    "unfair_dataset = choices(events, p_unfair, k=N)\n",
    "\n",
    "# Print summary statistics for sampled distributions\n",
    "print(f\"Fair coin:\\tP(X=Heads) {fair_dataset.count('H')/N}\")\n",
    "print(f\"Unfair coin:\\tP(X=Heads) {unfair_dataset.count('H')/N}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Shannon information content of an outcome $x$ is given by:\n",
    "\n",
    "$$\n",
    "h(x) = \\log_2{\\frac{1}{P(x)}}\n",
    "$$\n",
    "\n",
    "e.g. $x$ is a sinus rhythm finding of \"normal sinus rhythm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entropy of an ensemble $X$ is given by:\n",
    "\n",
    "$$\n",
    "H(X) = \\sum_x{P(x)\\log_2{\\frac{1}{P(x)}}}\n",
    "$$\n",
    "\n",
    "e.g. $X$ is the ensemble of sinus rhythm findings in dataset (normal, brady, tachy, other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    \"\"\"Calculate the entropy (bits) of a discrete probability distribution. \n",
    "\n",
    "    `p` is a list of probabilities of each discrete event.\n",
    "    \"\"\"\n",
    "    return sum(p[i] * -log2(p[i]) for i in range(len(p)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relative entropy between two probability distributions $P(x)$ and $Q(x)$ is also known as the Kullback-Leibler divergence.\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}(P||Q) = \\sum_x{P(x) \\log_2{\\frac{P(x)}{Q(x)}} }\n",
    "$$\n",
    "\n",
    "e.g. $P(x)$ and $Q(x)$ are both labelled for sinus rhythm outcomes.\n",
    "\n",
    "For multi-class dataset, each class will have a different entropy/divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(p, q):\n",
    "    return sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32192809488736235"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate exact KL-divergence between fair and unfair coin distributions\n",
    "kl_divergence(p_fair, p_unfair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27143927102495186"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In reality, we don't know the exact probability of events, so we need to \n",
    "# estimate these probabilities from the sampled distributions\n",
    "kl_divergence(\n",
    "    [fair_dataset.count('H')/N, fair_dataset.count('T')/N],\n",
    "    [unfair_dataset.count('H')/N, unfair_dataset.count('T')/N],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.454399071966485"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example with 3 events instead of 2 (heads and tails)\n",
    "kl_divergence(\n",
    "    [0.3, 0.3, 0.4],\n",
    "    [0.4, 0.5, 0.1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy([0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy([1/3, 1/3, 1/3]) == log2(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy([1/4, 1/4, 1/4, 1/4]) == log2(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.584962500721156"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log2(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8812908992306927"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The unfair coin is \"more predictable\" (is most likely to be heads), so there\n",
    "# is less uncertainty -> lower entropy.\n",
    "entropy([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4854752972273344"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# More events -> more bits of entropy.\n",
    "entropy([0.3, 0.5, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4150374992788438"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-log2(0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc resources\n",
    "\n",
    "https://machinelearningmastery.com/divergence-between-probability-distributions/\n",
    "\n",
    "[A Short Introduction to Entropy, Cross-Entropy and KL-Divergence](https://www.youtube.com/watch?v=ErfnhcEV1O8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
